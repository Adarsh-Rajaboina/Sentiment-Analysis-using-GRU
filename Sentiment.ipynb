{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1JyQY-FZtdTW30xORcdWPEIFhCV9QIqqB","authorship_tag":"ABX9TyNxXpAWuYBpa4HIARSCpIqJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":955},"id":"MRH1WpVnYzcD","executionInfo":{"status":"ok","timestamp":1753189270610,"user_tz":-330,"elapsed":171140,"user":{"displayName":"Sasank anatha pavan Pulipati","userId":"10807294054000539491"}},"outputId":"36d41dce-4b6d-4649-9e63-6ed61c6691e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded Data.csv\n","Calculated Class Weights: {0: np.float64(1.0596899224806202), 1: np.float64(0.9466759002770083)}\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_2\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m1,280,000\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m49,280\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m65,664\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m82,048\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n","│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n","│                     │                   │            │ global_max_pooli… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m49,280\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n","│                     │                   │            │ global_max_pooli… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,526,401\u001b[0m (5.82 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,526,401</span> (5.82 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,526,401\u001b[0m (5.82 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,526,401</span> (5.82 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 246ms/step - accuracy: 0.4764 - loss: 0.7000 - val_accuracy: 0.4994 - val_loss: 0.6935\n","Epoch 2/20\n","\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 240ms/step - accuracy: 0.6087 - loss: 0.6755 - val_accuracy: 0.4641 - val_loss: 0.6977\n","Epoch 3/20\n","\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 233ms/step - accuracy: 0.8035 - loss: 0.5828 - val_accuracy: 0.4970 - val_loss: 0.7198\n","Epoch 4/20\n","\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 238ms/step - accuracy: 0.9853 - loss: 0.2118 - val_accuracy: 0.5189 - val_loss: 0.7906\n","\n","Final Test Accuracy: 49.94%\n","\n","--- Making Final Predictions ---\n","Text: \"A 'hindrance to operations': extracts from the leaked reports\" --> Sentiment: Negative (Confidence: 50.82%)\n","Text: \"Stock prices soared after strong earnings report.\" --> Sentiment: Negative (Confidence: 50.06%)\n","Text: \"Lessons of law's hard heart\" --> Sentiment: Positive (Confidence: 50.04%)\n","Text: \"Victory and celebration in the city\" --> Sentiment: Negative (Confidence: 50.86%)\n","Text: \"you are fired .\" --> Sentiment: Negative (Confidence: 50.37%)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils import class_weight\n","import re\n","import os\n","\n","# --- NEW: SETTING RANDOM SEEDS FOR REPRODUCIBILITY ---\n","# This is the most important step to ensure you get the same result every time.\n","os.environ['PYTHONHASHSEED'] = str(42)\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","\n","# Import the specific Keras components needed for our advanced model\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, concatenate\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","# Import Tokenizer and pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","# --- 1. Data Loading and Preprocessing ---\n","try:\n","    df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks(DL)/datasets/Data.csv\", encoding=\"ISO-8859-1\")\n","    print(\"Successfully loaded Data.csv\")\n","except FileNotFoundError:\n","    print(\"Error: 'Data.csv' not found. Please make sure you have uploaded the file to your Colab session.\")\n","    exit()\n","\n","text_columns = [f'Top{i}' for i in range(1, 26)]\n","df['Combined_Text'] = df[text_columns].astype(str).agg(' '.join, axis=1)\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","df['Combined_Text'] = df['Combined_Text'].apply(clean_text)\n","df = df[['Combined_Text', 'Label']].dropna()\n","\n","X = df['Combined_Text'].values\n","y = df['Label'].values\n","\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","# --- 2. Handle Class Imbalance ---\n","class_weights_values = class_weight.compute_class_weight(\n","    'balanced', classes=np.unique(y_encoded), y=y_encoded\n",")\n","class_weights = dict(enumerate(class_weights_values))\n","print(f\"Calculated Class Weights: {class_weights}\")\n","\n","# --- 3. Tokenization and Padding ---\n","MAX_WORDS = 10000\n","MAX_LEN = 200\n","\n","tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n","tokenizer.fit_on_texts(X)\n","sequences = tokenizer.texts_to_sequences(X)\n","X_padded = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_padded, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",")\n","\n","# --- 4. Build the Advanced Multi-Branch 1D CNN Model ---\n","inputs = Input(shape=(MAX_LEN,))\n","embedding_layer = Embedding(input_dim=MAX_WORDS, output_dim=128)(inputs)\n","\n","convs = []\n","kernel_sizes = [3, 4, 5]\n","\n","for kernel_size in kernel_sizes:\n","    conv = Conv1D(filters=128, kernel_size=kernel_size, activation='relu')(embedding_layer)\n","    pool = GlobalMaxPooling1D()(conv)\n","    convs.append(pool)\n","\n","merged = concatenate(convs)\n","dense1 = Dense(128, activation='relu')(merged)\n","dropout1 = Dropout(0.5)(dense1)\n","outputs = Dense(1, activation='sigmoid')(dropout1)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","\n","# --- 5. Compile the Model ---\n","model.compile(\n","    optimizer=Adam(learning_rate=0.001),\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","model.summary()\n","\n","# --- 6. Train the Model ---\n","early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","\n","history = model.fit(\n","    X_train,\n","    y_train,\n","    epochs=20,\n","    validation_data=(X_test, y_test),\n","    batch_size=32,\n","    class_weight=class_weights,\n","    callbacks=[early_stop]\n",")\n","\n","# --- 7. Evaluation and Prediction ---\n","loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"\\nFinal Test Accuracy: {accuracy * 100:.2f}%\")\n","\n","def predict_sentiment(text):\n","    cleaned_text = clean_text(text)\n","    sequence = tokenizer.texts_to_sequences([cleaned_text])\n","    padded = pad_sequences(sequence, maxlen=MAX_LEN, padding='post', truncating='post')\n","    prediction = model.predict(padded, verbose=0)[0][0]\n","    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n","    confidence = prediction if sentiment == \"Positive\" else 1 - prediction\n","    print(f\"Text: \\\"{text}\\\" --> Sentiment: {sentiment} (Confidence: {confidence * 100:.2f}%)\")\n","\n","print(\"\\n--- Making Final Predictions ---\")\n","predict_sentiment(\"A 'hindrance to operations': extracts from the leaked reports\")\n","predict_sentiment(\"Stock prices soared after strong earnings report.\")\n","predict_sentiment(\"Lessons of law's hard heart\")\n","predict_sentiment(\"Victory and celebration in the city\")\n","predict_sentiment(\"you are fired .\")"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils import class_weight\n","import re\n","import os\n","import random\n","\n","# --- Reproducibility ---\n","seed = 42\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","random.seed(seed)\n","\n","# --- Preprocessing function ---\n","def clean_text(text):\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove urls\n","    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # keep only letters\n","    text = text.lower()\n","    return text\n","\n","# Example dataset load\n","df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks(DL)/datasets/Data.csv\", encoding=\"ISO-8859-1\")  # <-- replace with your dataset\n","\n","# Combine 'Top' columns into 'Combined_Text'\n","text_columns = [f'Top{i}' for i in range(1, 26)]\n","df['Combined_Text'] = df[text_columns].astype(str).agg(' '.join, axis=1)\n","\n","df['Combined_Text'] = df['Combined_Text'].apply(clean_text)\n","\n","# Encode labels\n","le = LabelEncoder()\n","df['Label'] = le.fit_transform(df['Label'])  # 0=Negative, 1=Positive\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['Combined_Text'], df['Label'], test_size=0.2, random_state=seed, stratify=df['Label']\n",")\n","\n","# Tokenization\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","MAX_VOCAB = 20000\n","MAX_LEN = 100\n","\n","tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_LEN)\n","X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_LEN)\n","\n","# Class weights (handles imbalance)\n","weights = class_weight.compute_class_weight(\n","    class_weight=\"balanced\",\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","class_weights = dict(enumerate(weights))\n","\n","# --- Model Architecture with GRU ---\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout\n","\n","EMBED_DIM = 100\n","\n","model = Sequential([\n","    Embedding(input_dim=MAX_VOCAB, output_dim=EMBED_DIM, input_length=MAX_LEN),\n","    Bidirectional(GRU(128, return_sequences=True)),\n","    Dropout(0.5),\n","    Bidirectional(GRU(64)),\n","    Dense(64, activation='relu'),\n","    Dropout(0.3),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","    metrics=['accuracy']\n",")\n","\n","# --- Callbacks ---\n","early_stop = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=3, restore_best_weights=True\n",")\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_loss', factor=0.5, patience=2\n",")\n","\n","# --- Training ---\n","history = model.fit(\n","    X_train_seq, y_train,\n","    validation_split=0.2,\n","    epochs=20,\n","    batch_size=64,\n","    class_weight=class_weights,\n","    callbacks=[early_stop, reduce_lr],\n","    verbose=1\n",")\n","\n","# --- Evaluation ---\n","loss, acc = model.evaluate(X_test_seq, y_test, verbose=0)\n","print(f\"\\nFinal Test Accuracy: {acc * 100:.2f}%\")\n","\n","# --- Prediction function ---\n","def predict_sentiment(text):\n","    text = clean_text(text)\n","    seq = tokenizer.texts_to_sequences([text])\n","    padded = pad_sequences(seq, maxlen=MAX_LEN)\n","    prediction = model.predict(padded)[0][0]\n","    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n","    confidence = prediction if sentiment == \"Positive\" else 1 - prediction\n","    print(f\"Text: \\\"{text}\\\" --> Sentiment: {sentiment} (Confidence: {confidence * 100:.2f}%)\")\n","\n","# --- Test Predictions ---\n","print(\"\\n--- Making Final Predictions ---\")\n","predict_sentiment(\"A 'hindrance to operations': extracts from the leaked reports\")\n","predict_sentiment(\"Stock prices soared after strong earnings report.\")\n","predict_sentiment(\"Lessons of law's hard heart\")\n","predict_sentiment(\"Victory and celebration in the city\")\n","predict_sentiment(\"you are fired .\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sX5iOizWgYwa","executionInfo":{"status":"ok","timestamp":1757858648717,"user_tz":-330,"elapsed":162439,"user":{"displayName":"Sasank anatha pavan Pulipati","userId":"10807294054000539491"}},"outputId":"942c03ce-95fb-4a08-b53c-3a501d1a2032"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 871ms/step - accuracy: 0.5479 - loss: 0.6921 - val_accuracy: 0.4802 - val_loss: 0.6935 - learning_rate: 0.0010\n","Epoch 2/20\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 823ms/step - accuracy: 0.5519 - loss: 0.6789 - val_accuracy: 0.4924 - val_loss: 0.8230 - learning_rate: 0.0010\n","Epoch 3/20\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 938ms/step - accuracy: 0.8456 - loss: 0.3688 - val_accuracy: 0.5137 - val_loss: 1.6114 - learning_rate: 0.0010\n","Epoch 4/20\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 833ms/step - accuracy: 0.9839 - loss: 0.0385 - val_accuracy: 0.5198 - val_loss: 1.7073 - learning_rate: 5.0000e-04\n","\n","Final Test Accuracy: 46.77%\n","\n","--- Making Final Predictions ---\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","Text: \"a  hindrance to operations   extracts from the leaked reports\" --> Sentiment: Positive (Confidence: 50.24%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n","Text: \"stock prices soared after strong earnings report \" --> Sentiment: Positive (Confidence: 50.29%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n","Text: \"lessons of law s hard heart\" --> Sentiment: Positive (Confidence: 50.21%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n","Text: \"victory and celebration in the city\" --> Sentiment: Positive (Confidence: 50.28%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n","Text: \"you are fired  \" --> Sentiment: Positive (Confidence: 50.30%)\n"]}]},{"cell_type":"code","source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B5AjsUHdgZwn","executionInfo":{"status":"ok","timestamp":1759075429027,"user_tz":-330,"elapsed":541622,"user":{"displayName":"Sasank anatha pavan Pulipati","userId":"10807294054000539491"}},"outputId":"136f6406-47c2-4223-a049-808f0ea88639"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-09-28 15:54:47--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2025-09-28 15:54:47--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2025-09-28 15:54:47--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n","\n","2025-09-28 15:57:27 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils import class_weight\n","import re\n","import os\n","import random\n","\n","# --- Reproducibility ---\n","seed = 42\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","random.seed(seed)\n","\n","# --- Preprocessing function ---\n","def clean_text(text):\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove urls\n","    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # keep only letters\n","    text = text.lower()\n","    return text\n","\n","# --- Load dataset ---\n","df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks(DL)/datasets/Data.csv\", encoding=\"ISO-8859-1\")\n","\n","# Combine 'Top' columns into one text column\n","text_columns = [f'Top{i}' for i in range(1, 26)]\n","df['Combined_Text'] = df[text_columns].astype(str).agg(' '.join, axis=1)\n","df['Combined_Text'] = df['Combined_Text'].apply(clean_text)\n","\n","# Encode labels\n","le = LabelEncoder()\n","df['Label'] = le.fit_transform(df['Label'])  # 0=Negative, 1=Positive\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['Combined_Text'], df['Label'], test_size=0.2, random_state=seed, stratify=df['Label']\n",")\n","\n","# --- Tokenization ---\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","MAX_VOCAB = 20000\n","MAX_LEN = 100\n","\n","tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_LEN)\n","X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_LEN)\n","\n","# --- Class weights (handles imbalance) ---\n","weights = class_weight.compute_class_weight(\n","    class_weight=\"balanced\",\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","class_weights = dict(enumerate(weights))\n","\n","# --- Download & Load GloVe embeddings ---\n","\n","embedding_index = {}\n","with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        vector = np.asarray(values[1:], dtype=\"float32\")\n","        embedding_index[word] = vector\n","\n","embedding_matrix = np.zeros((MAX_VOCAB, 100))\n","for word, i in tokenizer.word_index.items():\n","    if i < MAX_VOCAB:\n","        embedding_vector = embedding_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","# --- Model Architecture with GRU + GloVe ---\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout\n","from tensorflow.keras.regularizers import l2\n","\n","model = Sequential([\n","    Embedding(input_dim=MAX_VOCAB, output_dim=100, weights=[embedding_matrix],\n","              input_length=MAX_LEN, trainable=False),\n","    Dropout(0.3),\n","    Bidirectional(GRU(64, return_sequences=True, kernel_regularizer=l2(1e-4))),\n","    Dropout(0.4),\n","    Bidirectional(GRU(32, kernel_regularizer=l2(1e-4))),\n","    Dense(32, activation='relu'),\n","    Dropout(0.3),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n","    metrics=['accuracy']\n",")\n","\n","# --- Callbacks ---\n","early_stop = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=5, restore_best_weights=True\n",")\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_loss', factor=0.5, patience=3\n",")\n","\n","# --- Training ---\n","history = model.fit(\n","    X_train_seq, y_train,\n","    validation_split=0.2,\n","    epochs=25,\n","    batch_size=32,\n","    class_weight=class_weights,\n","    callbacks=[early_stop, reduce_lr],\n","    verbose=1\n",")\n","\n","# --- Evaluation ---\n","loss, acc = model.evaluate(X_test_seq, y_test, verbose=0)\n","print(f\"\\nFinal Test Accuracy: {acc * 100:.2f}%\")\n","\n","# --- Prediction function ---\n","def predict_sentiment(text):\n","    text = clean_text(text)\n","    seq = tokenizer.texts_to_sequences([text])\n","    padded = pad_sequences(seq, maxlen=MAX_LEN)\n","    prediction = model.predict(padded)[0][0]\n","    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n","    confidence = prediction if sentiment == \"Positive\" else 1 - prediction\n","    print(f\"Text: \\\"{text}\\\" --> Sentiment: {sentiment} (Confidence: {confidence * 100:.2f}%)\")\n","\n","# --- Test Predictions ---\n","print(\"\\n--- Making Final Predictions ---\")\n","predict_sentiment(\"A 'hindrance to operations': extracts from the leaked reports\")\n","predict_sentiment(\"Stock prices soared after strong earnings report.\")\n","predict_sentiment(\"Lessons of law's hard heart\")\n","predict_sentiment(\"Victory and celebration in the city\")\n","predict_sentiment(\"you are fired .\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yTvJFrhPkrkH","executionInfo":{"status":"ok","timestamp":1759075897770,"user_tz":-330,"elapsed":468736,"user":{"displayName":"Sasank anatha pavan Pulipati","userId":"10807294054000539491"}},"outputId":"6eb5800e-c19a-452d-acf9-1fb6083466dc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 225ms/step - accuracy: 0.4758 - loss: 0.7530 - val_accuracy: 0.5198 - val_loss: 0.7404 - learning_rate: 1.0000e-04\n","Epoch 2/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 206ms/step - accuracy: 0.5082 - loss: 0.7397 - val_accuracy: 0.5107 - val_loss: 0.7396 - learning_rate: 1.0000e-04\n","Epoch 3/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 217ms/step - accuracy: 0.4892 - loss: 0.7405 - val_accuracy: 0.5091 - val_loss: 0.7386 - learning_rate: 1.0000e-04\n","Epoch 4/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 204ms/step - accuracy: 0.4873 - loss: 0.7401 - val_accuracy: 0.5061 - val_loss: 0.7377 - learning_rate: 1.0000e-04\n","Epoch 5/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 200ms/step - accuracy: 0.5071 - loss: 0.7379 - val_accuracy: 0.5000 - val_loss: 0.7370 - learning_rate: 1.0000e-04\n","Epoch 6/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 204ms/step - accuracy: 0.4770 - loss: 0.7375 - val_accuracy: 0.5091 - val_loss: 0.7365 - learning_rate: 1.0000e-04\n","Epoch 7/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - accuracy: 0.5280 - loss: 0.7346 - val_accuracy: 0.5168 - val_loss: 0.7360 - learning_rate: 1.0000e-04\n","Epoch 8/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 201ms/step - accuracy: 0.5137 - loss: 0.7321 - val_accuracy: 0.5137 - val_loss: 0.7352 - learning_rate: 1.0000e-04\n","Epoch 9/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 208ms/step - accuracy: 0.5279 - loss: 0.7292 - val_accuracy: 0.5274 - val_loss: 0.7344 - learning_rate: 1.0000e-04\n","Epoch 10/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 204ms/step - accuracy: 0.5055 - loss: 0.7319 - val_accuracy: 0.5030 - val_loss: 0.7343 - learning_rate: 1.0000e-04\n","Epoch 11/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - accuracy: 0.5420 - loss: 0.7287 - val_accuracy: 0.5000 - val_loss: 0.7342 - learning_rate: 1.0000e-04\n","Epoch 12/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - accuracy: 0.5425 - loss: 0.7243 - val_accuracy: 0.5152 - val_loss: 0.7324 - learning_rate: 1.0000e-04\n","Epoch 13/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 203ms/step - accuracy: 0.5216 - loss: 0.7273 - val_accuracy: 0.5076 - val_loss: 0.7319 - learning_rate: 1.0000e-04\n","Epoch 14/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 198ms/step - accuracy: 0.5422 - loss: 0.7239 - val_accuracy: 0.5137 - val_loss: 0.7314 - learning_rate: 1.0000e-04\n","Epoch 15/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 199ms/step - accuracy: 0.5508 - loss: 0.7240 - val_accuracy: 0.5107 - val_loss: 0.7312 - learning_rate: 1.0000e-04\n","Epoch 16/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 198ms/step - accuracy: 0.5621 - loss: 0.7229 - val_accuracy: 0.5000 - val_loss: 0.7309 - learning_rate: 1.0000e-04\n","Epoch 17/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - accuracy: 0.5380 - loss: 0.7242 - val_accuracy: 0.4909 - val_loss: 0.7298 - learning_rate: 1.0000e-04\n","Epoch 18/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 205ms/step - accuracy: 0.5529 - loss: 0.7210 - val_accuracy: 0.5076 - val_loss: 0.7302 - learning_rate: 1.0000e-04\n","Epoch 19/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 201ms/step - accuracy: 0.5398 - loss: 0.7198 - val_accuracy: 0.5168 - val_loss: 0.7288 - learning_rate: 1.0000e-04\n","Epoch 20/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 204ms/step - accuracy: 0.5675 - loss: 0.7132 - val_accuracy: 0.4985 - val_loss: 0.7298 - learning_rate: 1.0000e-04\n","Epoch 21/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 197ms/step - accuracy: 0.5637 - loss: 0.7148 - val_accuracy: 0.4985 - val_loss: 0.7281 - learning_rate: 1.0000e-04\n","Epoch 22/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 199ms/step - accuracy: 0.5614 - loss: 0.7171 - val_accuracy: 0.4893 - val_loss: 0.7298 - learning_rate: 1.0000e-04\n","Epoch 23/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 194ms/step - accuracy: 0.5732 - loss: 0.7144 - val_accuracy: 0.4954 - val_loss: 0.7289 - learning_rate: 1.0000e-04\n","Epoch 24/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 202ms/step - accuracy: 0.5408 - loss: 0.7173 - val_accuracy: 0.5015 - val_loss: 0.7290 - learning_rate: 1.0000e-04\n","Epoch 25/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - accuracy: 0.5862 - loss: 0.7076 - val_accuracy: 0.4954 - val_loss: 0.7300 - learning_rate: 5.0000e-05\n","\n","Final Test Accuracy: 48.48%\n","\n","--- Making Final Predictions ---\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788ms/step\n","Text: \"a  hindrance to operations   extracts from the leaked reports\" --> Sentiment: Negative (Confidence: 53.98%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n","Text: \"stock prices soared after strong earnings report \" --> Sentiment: Positive (Confidence: 50.61%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n","Text: \"lessons of law s hard heart\" --> Sentiment: Negative (Confidence: 54.13%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n","Text: \"victory and celebration in the city\" --> Sentiment: Positive (Confidence: 50.28%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n","Text: \"you are fired  \" --> Sentiment: Negative (Confidence: 50.93%)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils import class_weight\n","import re\n","import os\n","import random\n","\n","# --- Reproducibility ---\n","seed = 42\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","random.seed(seed)\n","\n","# --- Preprocessing function ---\n","def clean_text(text):\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove urls\n","    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # keep only letters\n","    text = text.lower()\n","    return text\n","\n","# --- Load dataset ---\n","df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks(DL)/datasets/Data.csv\", encoding=\"ISO-8859-1\")\n","\n","# Combine 'Top' columns into one text column\n","text_columns = [f'Top{i}' for i in range(1, 26)]\n","df['Combined_Text'] = df[text_columns].astype(str).agg(' '.join, axis=1)\n","df['Combined_Text'] = df['Combined_Text'].apply(clean_text)\n","\n","# Encode labels\n","le = LabelEncoder()\n","df['Label'] = le.fit_transform(df['Label'])  # 0=Negative, 1=Positive\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['Combined_Text'], df['Label'], test_size=0.2, random_state=seed, stratify=df['Label']\n",")\n","\n","# --- Tokenization ---\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","MAX_VOCAB = 50000\n","MAX_LEN = 200\n","\n","tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_LEN)\n","X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_LEN)\n","\n","# --- Class weights (handles imbalance) ---\n","weights = class_weight.compute_class_weight(\n","    class_weight=\"balanced\",\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","class_weights = dict(enumerate(weights))\n","\n","# --- Download & Load GloVe embeddings ---\n","\n","embedding_index = {}\n","with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        vector = np.asarray(values[1:], dtype=\"float32\")\n","        embedding_index[word] = vector\n","\n","embedding_matrix = np.zeros((MAX_VOCAB, 100))\n","for word, i in tokenizer.word_index.items():\n","    if i < MAX_VOCAB:\n","        embedding_vector = embedding_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","# --- Model Architecture with GRU + GloVe ---\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout\n","from tensorflow.keras.regularizers import l2\n","\n","model = Sequential([\n","    Embedding(input_dim=MAX_VOCAB, output_dim=100, weights=[embedding_matrix],\n","              input_length=MAX_LEN, trainable=True),\n","    Dropout(0.3),\n","    Bidirectional(GRU(64, return_sequences=True, kernel_regularizer=l2(1e-4))),\n","    Dropout(0.4),\n","    Bidirectional(GRU(32, kernel_regularizer=l2(1e-4))),\n","    Dense(32, activation='relu'),\n","    Dropout(0.3),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n","    metrics=['accuracy']\n",")\n","\n","# --- Callbacks ---\n","early_stop = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=5, restore_best_weights=True\n",")\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_loss', factor=0.5, patience=3\n",")\n","\n","# --- Training ---\n","history = model.fit(\n","    X_train_seq, y_train,\n","    validation_split=0.2,\n","    epochs=25,\n","    batch_size=32,\n","    class_weight=class_weights,\n","    callbacks=[early_stop, reduce_lr],\n","    verbose=1\n",")\n","\n","# --- Evaluation ---\n","loss, acc = model.evaluate(X_test_seq, y_test, verbose=0)\n","print(f\"\\nFinal Test Accuracy: {acc * 100:.2f}%\")\n","\n","# --- Prediction function ---\n","def predict_sentiment(text):\n","    text = clean_text(text)\n","    seq = tokenizer.texts_to_sequences([text])\n","    padded = pad_sequences(seq, maxlen=MAX_LEN)\n","    prediction = model.predict(padded)[0][0]\n","    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n","    confidence = prediction if sentiment == \"Positive\" else 1 - prediction\n","    print(f\"Text: \\\"{text}\\\" --> Sentiment: {sentiment} (Confidence: {confidence * 100:.2f}%)\")\n","\n","# --- Test Predictions ---\n","print(\"\\n--- Making Final Predictions ---\")\n","predict_sentiment(\"A 'hindrance to operations': extracts from the leaked reports\")\n","predict_sentiment(\"Stock prices soared after strong earnings report.\")\n","predict_sentiment(\"Lessons of law's hard heart\")\n","predict_sentiment(\"Victory and celebration in the city\")\n","predict_sentiment(\"you are fired .\")"],"metadata":{"id":"KzOSdeLKod6R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758271584164,"user_tz":-330,"elapsed":1012519,"user":{"displayName":"Sasank anatha pavan Pulipati","userId":"10807294054000539491"}},"outputId":"514aa257-1438-481e-9b5c-6224dc21b36a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 473ms/step - accuracy: 0.4920 - loss: 0.7482 - val_accuracy: 0.4924 - val_loss: 0.7429 - learning_rate: 1.0000e-04\n","Epoch 2/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 466ms/step - accuracy: 0.5008 - loss: 0.7415 - val_accuracy: 0.4924 - val_loss: 0.7399 - learning_rate: 1.0000e-04\n","Epoch 3/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 465ms/step - accuracy: 0.5103 - loss: 0.7394 - val_accuracy: 0.5091 - val_loss: 0.7382 - learning_rate: 1.0000e-04\n","Epoch 4/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 464ms/step - accuracy: 0.5029 - loss: 0.7359 - val_accuracy: 0.5030 - val_loss: 0.7375 - learning_rate: 1.0000e-04\n","Epoch 5/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 462ms/step - accuracy: 0.5035 - loss: 0.7377 - val_accuracy: 0.5137 - val_loss: 0.7371 - learning_rate: 1.0000e-04\n","Epoch 6/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 548ms/step - accuracy: 0.5023 - loss: 0.7349 - val_accuracy: 0.5122 - val_loss: 0.7367 - learning_rate: 1.0000e-04\n","Epoch 7/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 508ms/step - accuracy: 0.5200 - loss: 0.7336 - val_accuracy: 0.4863 - val_loss: 0.7361 - learning_rate: 1.0000e-04\n","Epoch 8/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 497ms/step - accuracy: 0.5251 - loss: 0.7291 - val_accuracy: 0.4558 - val_loss: 0.7356 - learning_rate: 1.0000e-04\n","Epoch 9/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 520ms/step - accuracy: 0.5377 - loss: 0.7288 - val_accuracy: 0.5000 - val_loss: 0.7347 - learning_rate: 1.0000e-04\n","Epoch 10/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 481ms/step - accuracy: 0.5386 - loss: 0.7262 - val_accuracy: 0.5091 - val_loss: 0.7340 - learning_rate: 1.0000e-04\n","Epoch 11/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 553ms/step - accuracy: 0.5387 - loss: 0.7242 - val_accuracy: 0.5122 - val_loss: 0.7325 - learning_rate: 1.0000e-04\n","Epoch 12/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 478ms/step - accuracy: 0.5629 - loss: 0.7234 - val_accuracy: 0.4893 - val_loss: 0.7324 - learning_rate: 1.0000e-04\n","Epoch 13/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 517ms/step - accuracy: 0.5481 - loss: 0.7219 - val_accuracy: 0.4878 - val_loss: 0.7328 - learning_rate: 1.0000e-04\n","Epoch 14/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 484ms/step - accuracy: 0.5459 - loss: 0.7201 - val_accuracy: 0.4893 - val_loss: 0.7323 - learning_rate: 1.0000e-04\n","Epoch 15/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 540ms/step - accuracy: 0.5579 - loss: 0.7164 - val_accuracy: 0.4878 - val_loss: 0.7337 - learning_rate: 1.0000e-04\n","Epoch 16/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 488ms/step - accuracy: 0.5815 - loss: 0.7139 - val_accuracy: 0.4832 - val_loss: 0.7347 - learning_rate: 1.0000e-04\n","Epoch 17/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 542ms/step - accuracy: 0.5675 - loss: 0.7135 - val_accuracy: 0.4954 - val_loss: 0.7368 - learning_rate: 1.0000e-04\n","Epoch 18/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 476ms/step - accuracy: 0.5703 - loss: 0.7076 - val_accuracy: 0.4848 - val_loss: 0.7353 - learning_rate: 5.0000e-05\n","Epoch 19/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 521ms/step - accuracy: 0.5901 - loss: 0.7074 - val_accuracy: 0.4756 - val_loss: 0.7359 - learning_rate: 5.0000e-05\n","\n","Final Test Accuracy: 47.87%\n","\n","--- Making Final Predictions ---\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step\n","Text: \"a  hindrance to operations   extracts from the leaked reports\" --> Sentiment: Negative (Confidence: 54.79%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n","Text: \"stock prices soared after strong earnings report \" --> Sentiment: Positive (Confidence: 50.75%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","Text: \"lessons of law s hard heart\" --> Sentiment: Negative (Confidence: 51.46%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n","Text: \"victory and celebration in the city\" --> Sentiment: Negative (Confidence: 52.31%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n","Text: \"you are fired  \" --> Sentiment: Negative (Confidence: 50.44%)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils import class_weight\n","import re\n","import os\n","import random\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout\n","from tensorflow.keras.regularizers import l2\n","\n","# --- Reproducibility ---\n","seed = 42\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","random.seed(seed)\n","\n","# --- Preprocessing function ---\n","def clean_text(text):\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove urls\n","    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # keep only letters\n","    text = text.lower()\n","    return text\n","\n","# --- Load dataset ---\n","df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks(DL)/datasets/Data.csv\", encoding=\"ISO-8859-1\")\n","\n","# Combine 'Top' columns into one text column\n","text_columns = [f'Top{i}' for i in range(1, 26)]\n","df['Combined_Text'] = df[text_columns].astype(str).agg(' '.join, axis=1)\n","df['Combined_Text'] = df['Combined_Text'].apply(clean_text)\n","\n","# Encode labels\n","le = LabelEncoder()\n","df['Label'] = le.fit_transform(df['Label'])  # 0=Negative, 1=Positive\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['Combined_Text'], df['Label'], test_size=0.2, random_state=seed, stratify=df['Label']\n",")\n","\n","# --- Tokenization ---\n","MAX_VOCAB = 40000  # Increased vocabulary size\n","MAX_LEN = 150      # Increased max sequence length\n","\n","tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_LEN)\n","X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_LEN)\n","\n","# --- Class weights (handles imbalance) ---\n","weights = class_weight.compute_class_weight(\n","    class_weight=\"balanced\",\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","class_weights = dict(enumerate(weights))\n","\n","# --- Download & Load GloVe embeddings ---\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip\n","\n","embedding_index = {}\n","with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        vector = np.asarray(values[1:], dtype=\"float32\")\n","        embedding_index[word] = vector\n","\n","embedding_matrix = np.zeros((MAX_VOCAB, 100))\n","for word, i in tokenizer.word_index.items():\n","    if i < MAX_VOCAB:\n","        embedding_vector = embedding_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","# --- Model Architecture with GRU + GloVe ---\n","model = Sequential([\n","    Embedding(input_dim=MAX_VOCAB, output_dim=100, weights=[embedding_matrix],\n","              input_length=MAX_LEN, trainable=True), # Changed trainable to True\n","    Bidirectional(GRU(64, return_sequences=True, kernel_regularizer=l2(1e-4))),\n","    Dropout(0.4),\n","    Bidirectional(GRU(32, kernel_regularizer=l2(1e-4))),\n","    Dropout(0.4), # Increased Dropout\n","    Dense(32, activation='relu'),\n","    Dropout(0.3),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4), # Increased learning rate\n","    metrics=['accuracy']\n",")\n","\n","# --- Callbacks ---\n","early_stop = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=5, restore_best_weights=True\n",")\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_loss', factor=0.5, patience=3\n",")\n","\n","# --- Training ---\n","history = model.fit(\n","    X_train_seq, y_train,\n","    validation_split=0.2,\n","    epochs=25,\n","    batch_size=32,\n","    class_weight=class_weights,\n","    callbacks=[early_stop, reduce_lr],\n","    verbose=1\n",")\n","\n","# --- Evaluation ---\n","loss, acc = model.evaluate(X_test_seq, y_test, verbose=0)\n","print(f\"\\nFinal Test Accuracy: {acc * 100:.2f}%\")\n","\n","# --- Prediction function ---\n","def predict_sentiment(text):\n","    text = clean_text(text)\n","    seq = tokenizer.texts_to_sequences([text])\n","    padded = pad_sequences(seq, maxlen=MAX_LEN)\n","    prediction = model.predict(padded)[0][0]\n","    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n","    confidence = prediction if sentiment == \"Positive\" else 1 - prediction\n","    print(f\"Text: \\\"{text}\\\" --> Sentiment: {sentiment} (Confidence: {confidence * 100:.2f}%)\")\n","\n","# --- Test Predictions ---\n","print(\"\\n--- Making Final Predictions ---\")\n","predict_sentiment(\"A 'hindrance to operations': extracts from the leaked reports\")\n","predict_sentiment(\"Stock prices soared after strong earnings report.\")\n","predict_sentiment(\"Lessons of law's hard heart\")\n","predict_sentiment(\"Victory and celebration in the city\")\n","predict_sentiment(\"you are fired .\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRPJe5htDcG7","executionInfo":{"status":"ok","timestamp":1758270571630,"user_tz":-330,"elapsed":515634,"user":{"displayName":"Sasank anatha pavan Pulipati","userId":"10807294054000539491"}},"outputId":"c941af1f-3a26-4455-ca95-63f80fdd7a9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-09-19 08:21:05--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2025-09-19 08:21:05--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2025-09-19 08:21:05--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.05MB/s    in 2m 39s  \n","\n","2025-09-19 08:23:45 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 411ms/step - accuracy: 0.4922 - loss: 0.7559 - val_accuracy: 0.4482 - val_loss: 0.7407 - learning_rate: 5.0000e-04\n","Epoch 2/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 393ms/step - accuracy: 0.4890 - loss: 0.7400 - val_accuracy: 0.4817 - val_loss: 0.7361 - learning_rate: 5.0000e-04\n","Epoch 3/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 368ms/step - accuracy: 0.5120 - loss: 0.7327 - val_accuracy: 0.5198 - val_loss: 0.7304 - learning_rate: 5.0000e-04\n","Epoch 4/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 363ms/step - accuracy: 0.5427 - loss: 0.7252 - val_accuracy: 0.4802 - val_loss: 0.7387 - learning_rate: 5.0000e-04\n","Epoch 5/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 379ms/step - accuracy: 0.5427 - loss: 0.7165 - val_accuracy: 0.4787 - val_loss: 0.7360 - learning_rate: 5.0000e-04\n","Epoch 6/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 388ms/step - accuracy: 0.5722 - loss: 0.7048 - val_accuracy: 0.4863 - val_loss: 0.7618 - learning_rate: 5.0000e-04\n","Epoch 7/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 361ms/step - accuracy: 0.5943 - loss: 0.6923 - val_accuracy: 0.4832 - val_loss: 0.7681 - learning_rate: 2.5000e-04\n","Epoch 8/25\n","\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 399ms/step - accuracy: 0.6489 - loss: 0.6604 - val_accuracy: 0.4665 - val_loss: 0.8086 - learning_rate: 2.5000e-04\n","\n","Final Test Accuracy: 52.38%\n","\n","--- Making Final Predictions ---\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798ms/step\n","Text: \"a  hindrance to operations   extracts from the leaked reports\" --> Sentiment: Positive (Confidence: 51.64%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n","Text: \"stock prices soared after strong earnings report \" --> Sentiment: Positive (Confidence: 51.55%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n","Text: \"lessons of law s hard heart\" --> Sentiment: Positive (Confidence: 51.00%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n","Text: \"victory and celebration in the city\" --> Sentiment: Positive (Confidence: 52.64%)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n","Text: \"you are fired  \" --> Sentiment: Positive (Confidence: 53.52%)\n"]}]}]}